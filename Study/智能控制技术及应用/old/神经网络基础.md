# 神经网络基础

## 感知器模型

(1) 给定初始值 $W_i(0)$

(2) 输入一样本 $X=(x_i,\cdots,x_n,1)$ 和它的希望输出 $d$

(3) 计算实际输出

$$
Y(t)=f\Big(\sum_{i=1}^{n+1}W_i(t)x_i\Big)
$$


(4) 修正权

$$
W_i(t+1)=W_i(t)+\eta[d-Y(t)]x_i\quad i=1,2,\cdots,n+1
$$

式中，$0<\eta\leqslant 1$ 用于控制修正速度。

## 多层前向 BP 神经网络

假设神经网络的输入层、隐层和输出层的神经元数量分别为 $n$、$l$ 和 $m$，沿信息的传播方向，用 $In_j^{(i)}$、$Out_j^{(i)}$ 表示第 $i$ 层第 $j$ 个神经元的输入和输出。

输入层

$$
Out_i^{(1)} = In_i^{(1)} = x_i\quad i = 1 ,2 ,\cdots,n
$$

隐层

$$
In_j^{(2)}=\sum_{i=1}^n\left(w_{ij}^{(1)}Out_i^{(1)}-\theta_i\right)
$$

$$
Out_j^{(2)}=\phi(In_j^{(2)})\quad j=1,2,\cdots,l
$$

输出层

$$
y_{k} = Out_{k}^{(3)}= In_{k}^{(3)} = \sum_{j=1}^{l}w_{jk}^{(2)} Out_{j}^{(2)} \quad k= 1 ,2 ,\cdots,m
$$

![](PasteImage/2024-06-03-14-04-52.png)

$$
y_{k} = \sum_{j = 1}^{l}w_{jk}^{(2)} \phi\Big( \sum_{i = 1}^{n}w_{ij}^{(1)}x_{i} - \theta_{i}\Big) \quad k= 1 ,2 ,\cdots,m
$$
**BP学习算法**


(1) 依次取第 $k$ 组样本 $(\hat{X}_k,\hat{Y}_k), k=1,2,\cdots,m$，将 $\hat{X}_k$ 输入网络

(2) 依次计算目标函数，如果 $J<\varepsilon$，退出

$$
J=\frac12\sum_{k=1}^m\|\hat{Y}_k-Y_k\|^2
$$

(3) 计算

$$
\frac{\partial J_{k}}{\partial w_{ij}^{(1)}}=\sum_{i}\frac{\partial J_{k}}{\partial Y_{ki}}\frac{\partial Y_{ki}}{\partial Out_{j}^{(2)}}\frac{\partial Out_{j}^{(2)}}{\partial In_{j}^{(2)}}\frac{\partial In_{j}^{(2)}}{\partial w_{ij}^{(1)}}=-\sum_{i}(\hat{Y}_{ki}-Y_{ki})w_{j}^{(2)}f^{\prime}Out_{i}^{(1)}
$$

$$
\frac{\partial J_{k}}{\partial w_{j}^{(2)}} = \sum_{i} \frac{\partial J_{k}}{\partial Y_{ki}} \frac{\partial Y_{ki}}{\partial w_{j}^{(2)}} = - \sum_{i} ( \hat{Y}_{ki} - Y_{ki} ) Out_{j}^{(2)}
$$

(4) 计算

$$
\frac{\partial J}{\partial w} = \sum_{k = 1}^{m} \frac{\partial J_{k}}{\partial w}
$$

(5) 修正权值

$$
w(t+1)=w(t)-\eta\frac{\partial J}{\partial w(t)}
$$

## Hopfield 神经网络

![](PasteImage/2024-06-03-15-13-20.png)



